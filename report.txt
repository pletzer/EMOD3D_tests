1. Create a GIT repository for code and test inputs and implement 
       a structured environment for effective collaboration
=================================================================

Canterbury U gave github access of the code to Alex Pletzer. A 
CMake build system was added to the source code to enable rapid
experimentation of compiler switches. 

The CMake build system requires running

$ module load cmake
$ cmake [options] <src_dir> 
$ make

in the source or in some other directory. Hence, multiple versions of 
the same code can easily be built. 

The following configuration options can be used to select the compiler
options:

(a) cmake <src_dir>: default "Release" build

-- C compiler: mpcc
-- C compiler flags:  -qthreaded -qalias=noansi -qhalt=e -q64 -qkeyword=inline -O2 -qmaxmem=-1
-- Fortran compiler: mpxlf
-- Fortran compiler flags:  -qthreaded -qhalt=e -q64 -O2 -qmaxmem=-1


(b) cmake -D CMAKE_BUILD_TYPE=Power6 <src_dir>: targeting power6 
                                                architecture

-- C compiler flags:  -qthreaded -qalias=noansi -qhalt=e -q64 -qkeyword=inline -O2 -qmaxmem=-1 -qtune=pwr6 -qarch=pwr6
-- Fortran compiler flags:  -qthreaded -qhalt=e -q64 -O2 -qmaxmem=-1 -qtune=pwr6 -qarch=pwr6


(c) cmake -D CMAKE_BUILD_TYPE=OptimizedPower6 <src_dir>: more agressive
                                                         optimization for
                                                         power6
-- C compiler flags:  -qthreaded -qalias=noansi -qhalt=e -q64 -qkeyword=inline -O3 -qstrict -qmaxmem=-1 -qtune=pwr6 -qarch=pwr6
-- Fortran compiler flags:  -qthreaded -qhalt=e -q64 -O3 -qstrict -qmaxmem=-1 -qtune=pwr6 -qarch=pwr6

(d) cmake -D CMAKE_BUILD_TYPE=AggressivePower6 <src_dir>: even more aggressive 
           /Users/pletzer                                               optimization for power6
-- C compiler flags:  -qthreaded -qalias=noansi -qhalt=e -q64 -qkeyword=inline -O3 -qstrict -qmaxmem=-1 -qtune=pwr6 -qarch=pwr6 -qcache=auto -qhot -qsim
-- Fortran compiler flags:  -qthreaded -qhalt=e -q64 -O3 -qstrict -qmaxmem=-1 -qtune=pwr6 -qarch=pwr6 -qcache=auto -qhot -qsimd -qipa



2. Build Emod3d code on Fitzroy and verify - rerun small/large 
       input data and compare timings and output with BG/P
==============================================================

Small problem: 32 procs, 58mins on BG/P. Fitzroy results:

Optimization 				times		md5sum(seis-00001)

(a) default					7m26.099s   d587ee4811b1092a19679bb56b3d13bd		
(b) Power6					7m33.473s	d587ee4811b1092a19679bb56b3d13bd
(c) OptimizedPower6			7m4.622s	663339a0fc885906d2e018ffa0961401
(d) AggressivePower6		6m56.199s	663339a0fc885906d2e018ffa0961401

Large problem: 512 procs. Fitzroy results:

Optimization
(d) AggressivePower6		107m31.083s	c8469f9024c2ed78908f255fdde14165

3. Profile the code to identify performance bottlenecks
=======================================================

TAU instrumention applied to smaller (32 procs, Dec26 run) problem showed that 
roughly 30-50% of the time is spent in communication (MPI_sendrcv_replace and
exchange1_pvP3). About half the procs are spending more time in the tsteppP3 
routine that the other half, which is causing some imbalance.

NODE 0;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call 
---------------------------------------------------------------------------------------
100.0        0.005     8:25.897           1           1  505897214 .TAU application
100.0        2,688     8:25.897           1 1.46958E+06  505897209 int main(int, char **) C 
 47.7     3:55.965     4:01.486      600000      568320        402 void tsteppP3(int, int, float **, float **, struct fdcoefs *, int, int, int, struct nodeinfo *) C 
 29.7     2:07.436     2:30.316      600000      847889        251 void tstepvP3(int, int, float **, float **, float *, struct fdcoefs *, int, int, struct interface *, struct nodeinfo *) C 
 21.2       43,001     1:47.071       20000       60000       5354 size_t exchange1_pvP3(float *, float *, struct nodeinfo *, int, int) C 
 12.7     1:04.069     1:04.069       60000           0       1068 MPI_Sendrecv_replace() 
  2.6       12,496       12,904      600000       56181         22 void abs_xzbndP3(int, int, float **, float *, float *, float **, int, int, struct interface *, struct nodeinfo *) C 
  1.7        1,268        8,671       10000       23505        867 void tstepvbndP3(int, int, float **, float **, float *, struct fdcoefs *, int, int, str
    
4. Meet the performance target (3~4 times of BG/P speed for 
       the same number of cores used)
===========================================================

Sung to confirm that the results of large problem stored at 
/nesi/projects/nesi00213/Results/LPSim-2010Sept4_v1_Cantv1_64-h0.100_v3.04_Test/
are OK. Got 8x improvement in speed per processor by moving from BG/P to Fitzroy.

 
